{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Dependencies\" data-toc-modified-id=\"Dependencies-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Dependencies</a></span></li><li><span><a href=\"#Initiating-the-dataframes\" data-toc-modified-id=\"Initiating-the-dataframes-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Initiating the dataframes</a></span></li><li><span><a href=\"#Scraping-HLTV-into-the-dataframes\" data-toc-modified-id=\"Scraping-HLTV-into-the-dataframes-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Scraping HLTV into the dataframes</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extensive HLTV Scrape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "I will use pandas to save the data and export it as a csv file. I will use BeautifulSoup to parse the html content of the hltv pages. I will use requests to get the hltv pages. time for pausing the program between requests and re for regular expression matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T14:27:48.458888Z",
     "start_time": "2019-03-05T14:27:43.408506Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiating the dataframes\n",
    " - resultsdf for the results of every played map.\n",
    " - picksdf for the mappicks for every team in every veto process.\n",
    " - bansdf for the mapbans for every team in every veto process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-13T20:48:25.904209Z",
     "start_time": "2018-10-13T20:48:25.884209Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resultsdf = pd.DataFrame(columns=['matchcode', 'map', 'team1', 'team2', 'score1', 'score2', 'event', 'stars', 'date'])\n",
    "picksdf = pd.DataFrame(columns=['matchcode', 'team', 'map'])\n",
    "bansdf = pd.DataFrame(columns=['matchcode', 'team', 'map'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping HLTV into the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-13T22:54:26.698616Z",
     "start_time": "2018-10-13T22:54:26.431605Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def scrapePage(soup, resultsdf, picksdf, bansdf):\n",
    "    patternBan = re.compile(\"removed\")\n",
    "    patternPick = re.compile(\"picked\")\n",
    "    sublists = soup.find_all(\"div\", {\"class\": \"results-sublist\"})\n",
    "    \n",
    "    #for the first page when hltv displays feaured results\n",
    "    #for sublist in sublists[1:]:\n",
    "    for sublist in sublists:\n",
    "        try:\n",
    "            date = getDate(sublist.find(\"span\", {\"class\":\"standard-headline\"}).text.strip())\n",
    "            results = sublist.find_all(\"div\", {\"class\":\"result-con\"})\n",
    "            for result in results:\n",
    "                #time.sleep(.5)\n",
    "                url = \"https://www.hltv.org\" + result.find(\"a\", {\"class\":\"a-reset\"})['href']\n",
    "                team1Name = result.find(\"div\", {\"class\":\"team1\"}).text.strip()\n",
    "                team2Name = result.find(\"div\", {\"class\":\"team2\"}).text.strip()     \n",
    "                event = result.find(\"span\", {\"class\":\"event-name\"}).text.strip()\n",
    "                matchCode = result.find(\"a\", {\"class\":\"a-reset\"})['href'].split('/')[2]\n",
    "                stars = str(len(result.find_all(\"i\", {\"class\":\"fa fa-star star\"})))\n",
    "                innersoup = BeautifulSoup(requests.get(url).content, \"lxml\")\n",
    "                maps = innersoup.find_all(\"div\", {\"class\": \"mapholder\"})\n",
    "                for result in maps:\n",
    "                    mapname = result.find(\"div\", {'class':'mapname'}).text.strip()\n",
    "                    results = result.find(\"div\", {'class': 'results'})\n",
    "                    if results is not None:\n",
    "                        score1 = results.find_all(\"span\")[0].text.strip()\n",
    "                        score2 = results.find_all(\"span\")[2].text.strip()\n",
    "                        resultsdf = resultsdf.append({'matchcode': matchCode, 'map': mapname, 'team1': team1Name, 'team2':team2Name, 'score1':score1, 'score2':score2, 'event':event, 'stars':stars, 'date':date}, ignore_index=True)\n",
    "                try:\n",
    "                    vetos = innersoup.find_all(\"div\",{'class':'standard-box veto-box'})[1].find('div').find_all('div')\n",
    "                    for veto in vetos:\n",
    "                        string = veto.text.strip()\n",
    "                        if patternBan.search(string):\n",
    "                            splitstring = string.split(' removed ')\n",
    "                            bansdf = bansdf.append({'matchcode': matchCode, 'team': splitstring[0][3:], 'map':splitstring[-1]}, ignore_index=True)\n",
    "                        elif patternPick.search(string):\n",
    "                            splitstring = string.split(' picked ')\n",
    "                            picksdf = picksdf.append({'matchcode': matchCode, 'team': splitstring[0][3:], 'map':splitstring[-1]}, ignore_index=True)\n",
    "                except IndexError as e:\n",
    "                    print(\"No vetos for: \", matchCode)\n",
    "        except IOError as e:\n",
    "            print (e)\n",
    "    return (resultsdf, picksdf, bansdf)\n",
    "            \n",
    "            \n",
    "\n",
    "months = [\"default\", \"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]\n",
    "\n",
    "#string format = 'Results for September 10th 2018' returned format = '2018-09-10'\n",
    "def getDate(string):\n",
    "    splitString = string.split(' ')\n",
    "    year = str(splitString[4])\n",
    "    month = months.index(str(splitString[2]))\n",
    "    if (month < 10):\n",
    "        month = \"0\"+str(month)\n",
    "    day = int(splitString[3][:-2])\n",
    "    if (day < 10):\n",
    "        day = \"0\"+str(day)\n",
    "    return str(year) + \"-\" + str(month) + \"-\" + str(day)\n",
    "                                                   \n",
    "def doNothing():\n",
    "    return\n",
    "\n",
    "\n",
    "url = 'https://www.hltv.org/results?offset='\n",
    "#for i in range(0,1):\n",
    "    #time.sleep(1)  \n",
    "    #(resultsdf, picksdf, bansdf) = scrapePage(BeautifulSoup(requests.get(url+str(i)+'00').content, \"lxml\"), resultsdf, picksdf, bansdf)\n",
    "    #print('Scraped till page: ', i)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
